# Evolved-Intelligence Primer â€” Index
### A Structured Guide to the Evolutionary Architecture Behind L7A, L8A, and Self-Evolving AGI

This index provides a unified entry point into the Evolved-Intelligence Primer â€” a modular, LLM-friendly collection of foundational documents explaining how evolution, not retraining, produces stable, drift-invariant intelligence.

---

# ğŸ“š Primer Structure

The primer is organized as a conceptual staircase:
each section builds on the previous one, progressing from
the frequentist substrate to the full blueprint for self-evolving AGI.

---

## **00 â€” Overview**
A high-level introduction to the motivation and purpose of evolved-manifold intelligence.
- What the primer covers  
- Why evolution solves generalization  
- Why this matters for AGI  
- The overall roadmap  

ğŸ‘‰ *File:* `00_Overview.md`

---

## **01 â€” L7A Architecture**
The core system: evolved map surfaces, frequentist bins, binary decisions, and walk-forward survival.
- Frequentist substrate  
- Map surfaces as behavioral retinas  
- Differential probability fields  
- Evolutionary search  
- Abstention at the classifier layer  

ğŸ‘‰ *File:* `01_L7A_Architecture.md`

---

## **02 â€” Evolutionary Generalization**
Why evolution generalizes better than training, and why drift *improves* evolutionary systems instead of breaking them.
- Drift as the real training signal  
- Time as a natural selection operator  
- Why retraining fails  
- How evolution purifies structure over time  
- Why this principle is essential for AGI  

ğŸ‘‰ *File:* `02_Evolutionary_Generalization.md`

---

## **03 â€” Bin Surfaces and Behavioral Maps**
How temporal data becomes spatial structure â€” the fundamental perceptual mechanism of L7A.
- Bins, tallies, Laplace smoothing  
- 2-D trace projections  
- Differential surfaces  
- Multi-stream decorrelation  
- Noise cancellation across 96 channels  

ğŸ‘‰ *File:* `03_BinSurfaces_and_Maps.md`

---

## **04 â€” Confidence and Abstention**
Why intelligence requires knowing when **not** to answer.  
How L7A produces confidence and abstention naturally and why neural systems cannot.
- Emergent confidence  
- Bin agreement/disagreement  
- Perturbation stability  
- Abstention as the antidote to hallucination  
- Why abstention is non-negotiable for AGI safety  

ğŸ‘‰ *File:* `04_Confidence_and_Abstention.md`

---

## **05 â€” How to Build a Self-Evolving AGI**
The complete blueprint for constructing a drift-resistant, hallucination-free, self-evolving AGI using L7Aâ€™s architectural principles.
- Frequentist substrate  
- Evolutionary structural discovery  
- Drift as the purification mechanism  
- Multi-channel decorrelation  
- Higher-order evolved manifolds  
- Autopoietic (self-reconfiguring) evolution  

ğŸ‘‰ *File:* `05_How_to_Build_a_Self_Evolving_AGI.md`

---

# ğŸ” How to Use This Primer

This primer can be read:

- **Sequentially** (recommended)  
- **By topic** (the sections are modular)  
- **As a reference** for implementation and research  

Each document is intentionally short, clearly scoped, and tuned for ingestion by both humans and LLMs.

---

# ğŸ§© Connections to the Larger Project

The primer connects directly to three broader efforts:

### **1. Evolved-Manifold Intelligence (AGI Theory)**
The high-level conceptual framework for evolved generalization and cognitive structure.

### **2. L7A / L8A Forecasting Systems**
Real-world proof-of-concept systems demonstrating evolved generalization across 20 years of S&P 500 data.

### **3. The Evolved-AGI Implementation Blueprint**
The engineering path to build scalable self-evolving systems based on the substrate described here.

As the repo grows, these areas will be organized under:
- `/concepts/`
- `/code/`
- `/papers/`

---

### Attribution
*Concepts, architecture, and original system design by Christopher P. Wendling, with generative assistance and editorial support from OpenAIâ€™s ChatGPT.*
